# Between-study Heterogeneity

![](C:/Users/Admin/Documents/R/WORKING_DIRECTORY/Meta-Analyse Buch/bookdown-demo-master/schiffchen.jpg)

By now, we have already shown you how to pool effect sizes in a meta-analysis. In meta-analytic pooling, we aim to **synthesize the effects of many different studies into one single effect**. However, this makes only sense if we aren't comparing **Apples and Oranges**. For example, it could be the case that while the overall effect we calculate in the meta-analysis is **small**, there are still a few studies which report **very high** effect sizes. Such information is lost in the aggregate effect, but it is very important to know if all studies, or interventions, yield small effect sizes, or if there are exceptions.

It could also be the case that even some very **extreme effect sizes** were included in the meta-analysis, so-called **outliers**. Such outliers might have even distorted our overall effect, and it is important to how our overall effect would have looked without them.

The extent to which effect sizes vary within a meta-analysis is called **heterogeneity**. It is very important to assess heterogeneity in meta-analyses, as high heterogeneity could be caused by the fact that there are actually two or more **subgroups** of studies present in the data, which have a different true effect. Such information could be very valuable for **research**, because this might allow us to find certain interventions or populations for which effects are lower or higher.

From a statistical standpoint, high heterogeneity is also problematic. Very high heterogeneity could mean that the studies have nothing in common, and that there is no **"real" true effect behind our data**, meaning that it makes no sense to report the pooled effect at all [@borenstein2011].

```{block,type='rmdinfo'}
**The idea behind heterogeneity**

RÃ¼cker and colleagues [@rucker2008undue] name three types of heterogeneity in meta-analyses:

1.  *Clinical baseline heterogeneity*. These are differences between sample characteristics between the studies. For example, while one study might have included rather old people into their study, another might have recruited study participants who were mostly quite young.
2.  *Statistical heterogeneity*. This is the statistical heterogeneity we find in our collected effect size data. Such heterogeneity migh be either important from a clinical standpoint (e.g., when we don't know if a treatment is very or only marginally effective because the effects vary much from study to study), or from statistical standpoint (because it dilutes the confidence we have in our pooled effect)
3.  *Other sources of heterogeneity*, such as design-related heterogeneity.

Point 1. and 3. may be controlled for to some extent by restricting the scope of our search for studies to certain well-defined intervention types, populations, and outcomes.

Point 2., on the other hand, has to be assessed once we conducted the pooling of studies. This is what this chapter focuses on. 

```

```{block,type='rmdinfo'}
**Heterogeneity Measures**

There are **three types of heterogeneity measures** which are commonly used to assess the degree of heterogeneity. In the following examples, $k$ denotes the individual study, $K$ denotes all studies in our meta-analysis, $\hat \theta_k$ is the estimated effect of $k$ with a variance of $\hat \sigma^{2}_k$, and $w_k$ is the individual **weight** of the study (i.e., its *inverse variance*: $w_k = \frac{1}{\hat \sigma^{2}_k}$; see infobox in [Chapter 4.1.1](#fixed) for more details).

**1. Cochran's *Q* **

For Cochran's *Q*-statistic, is the **difference between the observed effect sizes and the fixed-effect model estimate** of the effect size, which is then **squared, weighted and summed**. 

$$ Q = \sum\limits_{k=1}^K w_k (\hat\theta_k  - \frac{\sum\limits_{k=1}^K w_k \hat\theta_k}{\sum\limits_{k=1}^K w_k})^{2}$$

**2. Higgin's & Thompson's *I*^2^ **

$I^{2}$ [@higgins2002quantifying] is the **percentage of variability** in the effect size estimates which is not caused by sampling error. It is derived from $Q$:

$$I^{2} = max \left\{0, \frac{Q-(K-1)}{Q}  \right\}$$

**3. Tau-squared**

$\tau^{2}$ is the between-study variance in our meta-analysis. As we show in [Chapter 4.2.1](#tau2), there are various proposed ways to calculate $\tau^{2}$
```

```{block, type='rmdachtung'}
**Which measure should i use?**

Generally, when we assess and report heterogeneity in a meta-analysis, we need a measure which is **robust, and not to easily influenced by statistical power**.

**Cochrane's *Q* ** increases both when the **number of studies** ($k$) increases, and when the **precision** (i.e., the sample size $N$ of a study) increases. Therefore, *Q* and weather it is **significant** highly depends on the size of your meta-analysis, and thus its statistical power. We should therefore not only rely on *Q* when assessing heterogeneity.

**I^2^** on the other hand, is not sensitive to changes in the number of studies in the analyses. I^2^ is therefore used extensively in medical and psychological research, especially since there is a **"rule of thumb"** to interpret it [@higgins2003measuring]:

* I^2^ = 25%: **low heterogeneity**
* I^2^ = 50%: **moderate heterogeneity**
* I^2^ = 75%: **substantial heterogeneity**

Despite its common use in the literature, I^2^ not always an adequate measure for heterogeneity either, because it still heavily depends on the **precision** of the included studies [@rucker2008undue; @borenstein2017basics]. As said before, $I^{2}$ is simply the amount of variability **not caused by sampling error**. If our studies become increasingly large, this sampling error tends to **zero**, while at the same time, $I^{2}$ tends to 100% simply because the single studies have greater $N$. Only relying on I^2^ is therefore not a good option either.

**Tau-squared**, on the other hand, is **insensitive** to the number of studies, **and** the precision. Yet, it is often hard to interpret how relevant our tau-squared is from a practicial standpoint.

**Prediction intervals** (like the ones we automatically calculated in [Chapter 4](#pool)) are a good way to overcome this limitation [@inthout2016plea], as they take our between-study variance into account. Prediction intervals give us a range for which we can **expect the effects of future studies to fall** based on **our present evidence in the meta-analysis**. If our prediction interval, for example, lies completely on the positive side favoring the intervention, we can be quite confident to say that **despite varying effects, the intervention might be at least in some way beneficial in all contexts we studied in the future**. If the confidence interval includes **zero**, we can be less sure about this, although it should be noted that **broad prediction intervals are quite common, especially in medicine and psychology**. 
```

## Assessing the heterogeneity of your pooled effect size

Thankfully, once you've already pooled your effects in meta-analysis using the `metagen()`, `metabin()`, or `metacont` function, it is very easy and straightforward to retrieve the **three most common heterogeneity measures** that we described before.

In [Chapter 4.2.2](#random.precalc), we already showed you how to conduct a **random-effect-model meta-analysis**. In this example, we stored our *results* in the object `m.hksj`, which we will use again here.


```{r, echo=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(knitr)
library(meta)
library(metafor)
load("Meta_Analysis_Data.RData")
madata<-Meta_Analysis_Data
load("metacont_data.RData")
metacont$Ne<-as.numeric(metacont$Ne)
metacont$Me<-as.numeric(metacont$Me)
metacont$Se<-as.numeric(metacont$Se)
metacont$Mc<-as.numeric(metacont$Mc)
metacont$Sc<-as.numeric(metacont$Sc)
m.hksj<-metagen(TE,
        seTE,
        data=madata,
        studlab=paste(Author),
        comb.fixed = FALSE,
        comb.random = TRUE,
        method.tau = "SJ",
        hakn = TRUE,
        prediction=TRUE,
        sm="SMD")
m.hksj.raw<-metacont(Ne,
        Me,
        Se,
        Nc,
        Mc,
        Sc,
        data=metacont,
        studlab=paste(Author),
        comb.fixed = FALSE,
        comb.random = TRUE,
        method.tau = "SJ",
        hakn = TRUE,
        prediction=TRUE,
        sm="SMD")
```

One way to get heterogeneity measures of my meta-analysis is to **print** the meta-analysis (in my case, `m.hksj`) output again.

```{r}
print(m.hksj)

```

We see that this output **already provides us with all three heterogeneity measures** (and even one more, *H*, which we'll not cover here).

* $\tau^{2}$, as we can see from the `tau^2` output, is **0.1337**.

* $I^{2}$ is printed next to `I^2`, and has the value **62.6%**, and a 95% confidence interval rangin from 37.9% to 77.5%.

* The value of $Q$ is displayed next to `Q` under `Test of heterogeneity:`.
As we can see, the value is **45.50**. In our case, this is highly significant (0.0002; see `p-value`).

* The **prediction interval** can be found next to `Prediction interval`. As we can see, the 95% interval ranges from **g=-0.2084** to **1.3954**.

How can we interpret the values of this example analysis? Well, all three of our indicators suggest that **moderate to substantial heterogeneity is present in our data**. Given the **broad prediction interval**, which stretches well below zero, we also cannot be overly confident that the positive effect we found for our interventions is robust in every context. It might be very well possible that the intervention does not yield positive effects in some future scenarios; even a small negative effect might be possible based on the evidence the meta-analysis gives us. Very high effect sizes, on the other hand, are possible too.

<br><br>

**When the measures are not displayed in my output**

Depending on how you changed the settings of the `metagen`, `metabin`, or `metacont`, it is possible that some of the measures are not displayed in your output. That's not a big deal, because all measures are stored in the object, no matter if they are immediately displayed or not.

To directly access one of the measures, we can to use `$` again (see [Chapter 3.3.1](#convertfactors)). We use this **in combination with our meta-analysis output object** to define which measure we want to see.

```{r,echo=FALSE}
library(kableExtra)
Code<-c("$Q","$pval.Q","$I2","$lower.I2","$upper.I2","$tau^2","$lower.predict","$upper.predict")
Measure<-c("Cochran's Q","The p-value for Cochran's Q","I-squared","The lower bound of the I-squared 95%CI","The upper bound of the I-squared 95%CI","Tau-squared","The lower bound of the 95% prediction interval","The upper bound of the 95% prediction interval")
m<-data.frame(Code,Measure)
names<-c("Code","Measure")
colnames(m)<-names
kable(m)
```
Here are a few exmaples for my `m.hksj` object. As you'll see, the output is **identical** to the one before.

```{r}
m.hksj$Q
```
```{r}
m.hksj$I2
```
```{r}
m.hksj$tau^2
```



## Define analyzed meta-analysis output

```{r}
influence.data<-m.hksj
```

```{r,eval=FALSE,warning=FALSE}
res <- rma(yi=TE, sei=seTE, measure="ZCOR", 
           data=influence.data, 
           method = "SJ", 
           test="knha")
inf <- influence(res)
influence.data.metainf<-metainf(influence.data)
influence.data.metainf$I2<-format(round(influence.data.metainf$I2,2),nsmall=2)
plot(inf)
baujat(influence.data)
forest(influence.data.metainf,
       sortvar=I2,
       rightcols = c("TE","ci","I2"),
       smlab = "Sorted by I-squared")
forest(influence.data.metainf,
       sortvar=TE,
       rightcols = c("TE","ci","I2"),
       smlab = "Sorted by Effect size")
```

```{r,echo=FALSE}
res <- rma(yi=TE, sei=seTE, measure="ZCOR", 
           data=influence.data, 
           method = "SJ", 
           test="knha")
inf <- influence(res)
influence.data.metainf<-metainf(influence.data)
influence.data.metainf$I2<-format(round(influence.data.metainf$I2,2),nsmall=2)
```

```{r,echo=FALSE,fig.align='center',fig.cap="Influence Analyses"}
plot(inf)
```
```{r,echo=FALSE, fig.align='center',fig.cap="Baujat Plot",fig.height=10,fig.width=10}
baujat(influence.data)
```

```{r,echo=FALSE, fig.align='center',fig.cap="Leave-One-Out-Analyses"}
forest(influence.data.metainf,
       sortvar=I2,
       rightcols = c("TE","ci","I2"),
       smlab = "Sorted by I-squared")
forest(influence.data.metainf,
       sortvar=TE,
       rightcols = c("TE","ci","I2"),
       smlab = "Sorted by Effect size")
```
