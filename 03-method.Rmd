# Pooling Effect Sizes

Now, let's get to the core of every Meta-Analysis: **pooling your effect sizes** to get one overall effect size estimate of the studies.

```{block,type='rmdinfo'}
When pooling effect sizes in Meta-Analysis, there are two approaches which we can use: the **Fixed-Effects-Model**, or the **Random-Effects-Model** [@borenstein2011]. There is an extensive debate on which model fits best in which context [@fleiss1993review], with no clear consensus in sight. Although it has been recommended to **only resort to the Random-Effects-Pooling model** in clinical psychology and the health sciences [@cuijpers2016meta], we will describe how to conduct both in R here.

Both of these models only require an **effect size**, and a **dispersion (variance)** estimate for each study, of which the inverse is taken. This is why the methods are often called **general inverse-variance methods**.
```

We will describe in-depth how to conduct meta-analyses in R with **continuous variables** (such as effect sizes), as these are the most common ones in psychology and the health science field. Later on, we will present briefly how to do meta-analyses with **binary outcome** data too, which might be important if you're focusing on prevention trials.

For these Meta-Analyses, we'll use the `meta` package [@schwarzer2007meta]. In [Section 2.1](#RStudio), we show how to install the package. Now, load the package from your library to proceed.

```{r, warning=FALSE,message=FALSE}
library(meta)
```

```{r, echo=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(knitr)
```

## Fixed-Effects-Model {#fixed}

### Pre-calculated effect size data

```{block, type='rmdinfo'}
**The idea behind the fixed-effects-model**

The fixed-effects-model assumes that all studies along with their effect sizes stem from a single homogeneous population [@borenstein2011]. To calculate the overall effect, we therefore average all effect sizes, but give studies with greater precision a higher weight. In this case, greater precision means that the study has a larger **N**, which leads to a smaller **Standard Error** of its effect size estimate.

For this weighing, we use the **inverse of the variance** $1/\hat\sigma^2_k$ of each study $k$. We then calculate a weighted average of all studies, our fixed effect size estimator $\hat\theta_F$:

\begin{equation}
\hat\theta_F = \frac{\sum\limits_{k=1}^K \hat\theta_k/ \hat\sigma^2_k}{\sum\limits_{k=1}^K 1/\hat\sigma^2_k}
\end{equation}

```

In [Chapter 3.1](#excel_preparation), we have described two ways your EXCEL spreadsheet for you Meta-Analysis data can look like:

* It can either be stored as the raw data (including the Mean, N, and SD of every study arm)
* Or it only contains the calculated effect sizes and the standard error (SE)

The functions to pool the results with a fixed-effects-model differ depending on which data format you used, but not much. First, let's assume you already have a dataset with the calucated effects and SE for each study. In my case, this is my `madata` dataset.

```{r,echo=FALSE}
load("Meta_Analysis_Data.RData")
madata<-Meta_Analysis_Data
```

```{r}
str(madata)
```

This dataset has **continuous outcome data**. As our effect sizes are already calculated, we can use the `meta::metagen` function. For this function, we can specify loads of parameters, all of which you can access by typing `?metagen` in your console once the `meta` package is loaded.
  
Here is a table with the most important parameters for our code:

```{r,echo=FALSE}
i<-c("TE","seTE","data=","studlab=paste()","comb.fixed=","comb.random","prediction=","sm=")
ii<-c("This tells R to use the TE column to retrieve the effect sizes for each study",
      "This tells R to use the seTE column to retrieve the standard error for each             study",
      "After =, paste the name of your dataset here",
      "This tells the function were the labels for each study are stored. If you named the spreadsheet columns as advised, this should be studlab=paste(Author)",
      "Weather to use a fixed-effects-model",
      "Weather to use a random-effects-model",
      "Weather to print a prediction interval for the effect of future studies based on present evidence","The summary measure we want to calculate. We can either calculate the mean difference (MD) or Hedges' g (SMD)")
ms<-data.frame(i,ii)
names<-c("Parameter", "Function")
colnames(ms)<-names
kable(ms)
```

Let's code our first fixed-effects-model Meta-Analysis. We we will give the results of this analysis the simple name `m`.

```{r}
m<-metagen(TE,
        seTE,
        data=madata,
        studlab=paste(Author),
        comb.fixed = TRUE,
        comb.random = FALSE,
        prediction=TRUE,
        sm="SMD")
m

```

We now see the results of our Meta-Analysis, including

* The **individual effect sizes** for each study, and their weight
* The total **number of included studies** (k)
* The **overall effect** (in our case, g=0.4805) and its confidence interval and p-value
* Measures of **between-study heterogeneity**, such as *tau^2^* or *I^2^* and a *Q*-test of heterogeneity

Using the `$` command, we can also have a look at various outputs directly. For example

```{r, eval=FALSE}
m$lower.I2
```

Gives us the lower 95% confidence interval for *I^2^*

```{r, echo=FALSE}
m$lower.I2
```

We can **save the results of the meta-analysis** to our working directory as a .txt-file using this command

```{r,eval=FALSE}
sink("results.txt")
print(m)
sink()
```

### Raw effect size data {#fixed.raw}

To conduct a fixed-effects-model Meta-Analysis from **raw data** (i.e, if your data has been prepared the way we describe in [Chapter 3.1.1](#excel_preparation)), we have to use the `meta::metacont()` function instead. The structure of the code however, looks quite similar.

```{r,echo=FALSE}
i<-c("Ne", "Me", "Se", "Nc", "Mc","Sc","data=","studlab=paste()","comb.fixed=","comb.random","prediction=","sm=")
ii<-c("The number of participants (N) in the intervention group",
      "The Mean (M) of the intervention group",
      "The Standard Deviation (SD) of the intervention group",
      "The number of participants (N) in the control group",
      "The Mean (M) of the control group",
      "The Standard Deviation (SD) of the control group",
      "After '=', paste the name of your dataset here",
      "This tells the function were the labels for each study are stored. If you named the spreadsheet columns as advised, this should be studlab=paste(Author)",
      "Weather to use a fixed-effects-model",
      "Weather to use a random-effects-model",
      "Weather to print a prediction interval for the effect of future studies based on present evidence","The summary measure we want to calculate. We can either calculate the mean difference (MD) or Hedges' g (SMD)")
ms2<-data.frame(i,ii)
names<-c("Parameter", "Function")
colnames(ms2)<-names
kable(ms2)
```

```{r,echo=FALSE,warning=FALSE}
load("metacont_data.RData")
metacont$Ne<-as.numeric(metacont$Ne)
metacont$Me<-as.numeric(metacont$Me)
metacont$Se<-as.numeric(metacont$Se)
metacont$Mc<-as.numeric(metacont$Mc)
metacont$Sc<-as.numeric(metacont$Sc)
```

For this purpose, i will use my dataset `metacont`, which contains the raw data of all studies i want to snythesize

```{r}
str(metacont)
```

Now, let's code the Meta-Analysis function, this time using the `meta::metacont` function, and my `metacont` dataset. I want to name my output `m.raw` now.

```{r}
m.raw<-metacont(Ne,
                Me,
                Se,
                Nc,
                Mc,
                Sc,
                data=metacont,
                studlab=paste(Author),
                comb.fixed = TRUE,
                comb.random = FALSE,
                prediction=TRUE,
                sm="SMD")
m.raw
```
 
 
```{block,type='rmdachtung'}
As you can see, all the calculated effect sizes are **negative** now, including the pooled effect. However, all studies report a positive outcome, meaning that the symptoms in the intervention group (e.g., of depression) were reduced. The negative orientation results from the fact that in **most clinical trials, lower scores indicate better outcomes** (e.g., less depression). It is no problem to report values like this: in fact, it is conventional. 

Some readers who are unfamiliar with meta-analysis, however, **might be confused** by this, so may consider changing the orientation of your values before you report them in your paper.
```

We can **save the results of the meta-analysis** to our working directory as a .txt-file using this command

```{r,eval=FALSE}
sink("results.txt")
print(m.raw)
sink()
```


## Random-Effects-Model {#random}

Previously, we showed how to perform a fixed-effects-model meta-analysis using the `meta:metagen` and `meta:metacont` functions.

However, we can only use the fixed-effects-model when we can assume that **all included studies come from the same population**. In practice this is hardly ever the case: interventions may vary in certain characteristics, the sample used in each study might be slightly different, or its methods. In this case, we cannot assume that all studies stem from one hypothesized "population" of studies. 

Same is the case once we detect **statistical heterogeneity** in our fixed-effect-model meta-analysis, as indicated by $I^{2}>0$.

So, it is very likely that you will actually use a random-effects-model for your meta-analysis. Thankfully, there's not much more we have to think about when conducting a random-effects-model meta-analysis in R instead of a fixed-effects-model meta-analysis.

```{block,type='rmdinfo'}
**The Idea behind the Random-Effects-Model**

In the Random-Effects-Model, we want to account for our assumption that the study effect estimates show more variance than when drawn from a single population [@schwarzer2015meta]. The random-effect-model works under the so-called **assumption of exchangeability**. 

This means that in Random-Effect-Model Meta-Analyses, we not only assume that effects of individual studies deviate from the true intervention effect of all studies due to sampling error, but that there is another source of variance introduced by the fact that the studies do not stem from one single population, but are drawn from a "universe" of populations. We therefore assume that there is not only one true effect size, but **a distribution of true effect sizes**. Were therefore want to estimate the mean of this distribution of true effect sizes.

The fixed-effects-model assumes that when the observed effect size $\hat\theta_k$ of an individual study $k$ deviates from the true effect size $\theta_F$, the only reason for this is that the estimate is burdened by (sampling) error $\epsilon_k$.

$$\hat\theta_k = \theta_F + \epsilon_k$$


While the random-effects-model assumes that, in addition, there is **a second source of error** $\zeta_k$.This second source of error is introduced by the fact that even the true effect size $\theta_k$ of our study $k$ is also only part of an over-arching distribution of true effect sizes with the mean $\mu$ [@borenstein2011]. 

```

```{r, echo=FALSE, fig.width=6,fig.height=4,fig.align='center'}
library(png)
library(grid)
img <- readPNG("C:/Users/Admin/Documents/R/WORKING_DIRECTORY/Meta-Analyse Buch/bookdown-demo-master/density.png")
grid.raster(img)
```
<p style="text-align: center;">
*An illustration of  parameters of the random-effects-model*
</p>
 
```{block,type='rmdinfo'}
The formula for the random-effects-model therefore looks like this:

$$\hat\theta_k = \mu + \epsilon_k + \zeta_k$$

When calculating a random-effects-model meta-analysis, where therefore also have to take the error $\zeta_k$ into account. To do this, we have to **estimate the variance of the distribution of true effect sizes**, which is denoted by $\tau^{2}$, or *tau^2^*. There are several estimators for $\tau^{2}$, all of which are implemented in `meta`. We will give your more details about them in the next section.
```

```{block,type='rmdachtung'}
Even though it is **conventional** to use random-effects-model meta-analyses in psychological outcome research, applying this model is **not undisputed**. The random-effects-model pays **more attention to small studies** when pooling the overall effect in a meta-analysis [@schwarzer2015meta]. Yet, small studies in particular are often fraught with **bias**. This is why some have argued that the fixed-effects-model should be nearly always preferred [@poole1999; @furukawa2003].
```

### Estimators for *tau^2^* in the random-effects-model

Operationally, conducting a random-effects-model meta-analysis in R is not so different from conducting a fixed-effects-model meta-analyis. Yet, we do have choose an estimator for $\tau^{2}$. Here are the estimators implemented in `meta`, which we can choose using the `method.tau` variable in our meta-analysis code.

```{r,echo=FALSE}
i<-c("DL","PM","REML","ML","HS","SJ","HE","EB")
ii<-c("DerSimonian-Laird","Paule-Mandel","Restricted Maximum-Likelihood","Maximum-likelihood","Hunter-Schmidt","Sidik-Jonkman","Hedges","Empirical Bayes")
ms<-data.frame(i,ii)
names<-c("Code", "Estimator")
colnames(ms)<-names
kable(ms)
```

```{block,type='rmdinfo'}
**Which estimator should i use?**

All of these estimators derive $\tau^{2}$ using a slightly different approach, leading to somewhat different pooled effect size estimates and confidence intervals. If one of these approaches is more or less biased often depends on the context, and parameters such as the number of studies $k$, the number of participants $n$ in each study, and how much $n$ varies from study to study, and how big $\tau^{2}$ is. 

An overview paper by Veroniki and colleagues [@veroniki2016methods] provides an excellent summary on current evidence which estimator might be more or less biased in which situation. The article is Open Access, and you can read it [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4950030/).

Especially in medical and psychological research, the by far most often used one is the **DerSimonian-Laird estimator** [@dersimonian1986meta]. Part of this widespread use might be attributable to the fact that programs such as *RevMan* or *Comprehensive Meta-Analysis* (older versions) only use this estimator. It is also the default option in our `meta` package in R. Simulation studies, however, have shown that the **Maximum-Likelihood**, **Sidik-Jonkman**, and **Empirical Bayes** estimators have better properties in estimating the between-study variance [@sidik2007comparison;@viechtbauer2005bias].
```

```{block,type='rmdinfo'}
**The Hartung-Knapp-Sidik-Jonkman method**
  
Another criticism of the **DerSimonian-Laird** method is that when estimating the variance of our pooled effect $var(\hat\theta_F)$, this method is very prone to producing false positives [@inthout2014hartung]. This especially the case when the **number of studies** is small, and when there is substantial **heterogeneity**[@hartung1999alternative;@hartung2001refined;@hartung2001tests;@follmann1999valid;@makambi2004effect]. Unfortunately, this is very often the case in when we do meta-analysis in the medical field or in psychology. This is quite a problem, as we don't want to find pooled effects to be statistically significant when in fact they are not!

The **Hartung-Knapp-Sidik-Jonkman (HKSJ) method** was thus proposed a way to produce more robust estimates of $var(\hat\theta_F)$. It has been shown that this method substantially outperforms the DerSimonian-Laird method in many cases [@inthout2014hartung]. The HKSJ method can also be very easily applied in R, while other programs don't have this option yet. This is another big plus of doing meta-analysis in R. The HKSJ usually leads to more **conservative** results, indicated by wider confidence intervals.
```

```{block,type='rmdachtung'}
It should be noted, however, that the HKSJ method is not uncontroversial. Some authors argue suggest that other (standard) pooling models should also be used **in addition** to the HKSJ as a **sensitivity analysis** [@wiksten2016hartung]. Jackson and colleagues [@jackson2017hartung] present four residual concerns with this method, which you may take into account before selecting your meta-analytic method. The paper can be read [here](https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7411).
```

### Pre-calculated effect size data

After all this input, you'll see that even random-effects-model meta-analyses are very easy to code in R. Compared to the fixed-effects-model [Chapter 4.1](#fixed), there's just three extra parameters we have to define. Especially, as we've described before, we have to tell R which **between-study-variance estimator** ($\tau^{2}$) we want to use, and if we want to use the **Knapp-Hartung-Sidik-Jonkman** adjustment.

**Here's a table of all parameters we have to define in our code to perform a random-effects-model meta-analysis with pre-calculated effect sizes**

```{r,echo=FALSE}
i<-c("TE","seTE","data=","studlab=paste()","comb.fixed=","comb.random=","method.tau=","hakn=", "prediction=","sm=")
ii<-c("This tells R to use the TE column to retrieve the effect sizes for each study",
      "This tells R to use the seTE column to retrieve the standard error for each             study",
      "After =, paste the name of your dataset here",
      "This tells the function were the labels for each study are stored. If you named the spreadsheet columns as advised, this should be studlab=paste(Author)",
      "Weather to use a fixed-effects-model",
      "Weather to use a random-effects-model. This has to be set to TRUE",
      "Which estimator to use for the between-study variance",
      "Weather to use the Knapp-Hartung-Sidik-Jonkman method",
      "Weather to print a prediction interval for the effect of future studies based on present evidence","The summary measure we want to calculate. We can either calculate the mean difference (MD) or Hedges' g (SMD)")
ms<-data.frame(i,ii)
names<-c("Parameter", "Function")
colnames(ms)<-names
kable(ms)
```

I will use my `madata` dataset again to do the meta-analysis. For illustrative purposes, let's use the Sidik-Jonkman estimator ("SJ") and the HKSJ method. To do this analysis, make sure that `meta` as well as `metafor` are loaded in R.

```{r,eval=FALSE,warning=FALSE}
library(meta)
library(metafor)
```

Now, let's code our random-effects-model meta-analysis. Remember, as our effect size data are precalculated, i'll use the `meta::metagen()` function.

```{r,echo=FALSE}
load("Meta_Analysis_Data.RData")
madata<-Meta_Analysis_Data
```

```{r}
m.hksj<-metagen(TE,
        seTE,
        data=madata,
        studlab=paste(Author),
        comb.fixed = FALSE,
        comb.random = TRUE,
        method.tau = "SJ",
        hakn = TRUE,
        prediction=TRUE,
        sm="SMD")
m.hksj

```
The output shows that our estimated effect is ***g*=0.5935**, and the 95% confidence interval stretches from *g*=0.39 to 0.80 (rounded).

It also becomes clear that this effect is different (and larger) than the one we found in the fixed-effects-model meta-analysis in [Chapter 4.1](#fixed) (*g*=0.48).

Let's compare this to the output using the **DerSimonian-Laird** estimator, and when setting `hakn=FALSE`. As this estimator is the **default**, i don't have to define `method.tau` this time.

```{r}
m.dl<-metagen(TE,
        seTE,
        data=madata,
        studlab=paste(Author),
        comb.fixed = FALSE,
        comb.random = TRUE,
        hakn = FALSE,
        prediction=TRUE,
        sm="SMD")
m.dl

```
We see that the overall effect size estimate using this estimator is similar to the previous one (*g*=0.57), but the confidence intervals **is narrower because we did not adjust them** using the HKSJ method.

```{r,echo=FALSE,fig.height=2}
TE<-c(m.hksj$TE.random,m.dl$TE.random)
seTE<-c(m.hksj$seTE.random,m.dl$seTE.random)
Method<-c("Knapp-Hartung-Sidik-Jonkman","DerSimonian-Laird")
frst.data<-data.frame(Method,TE,seTE)
m.frst<-metagen(TE,
        seTE,
        data=frst.data,
        studlab=paste(Method),
        comb.fixed = FALSE,
        comb.random = FALSE,
        hakn = FALSE,
        prediction=FALSE)
forest(m.frst,xlim = c(0.34,0.85))

```

### Raw effect size data {#random.raw}

I we use raw effect size data, such as the one stored in my `metacont` dataset, we can use the `meta::metacont` function again. The parameters stay the same as before.

```{r,echo=FALSE,warning=FALSE}
load("metacont_data.RData")
metacont$Ne<-as.numeric(metacont$Ne)
metacont$Me<-as.numeric(metacont$Me)
metacont$Se<-as.numeric(metacont$Se)
metacont$Mc<-as.numeric(metacont$Mc)
metacont$Sc<-as.numeric(metacont$Sc)
```

```{r}
m.hksj.raw<-metacont(Ne,
        Me,
        Se,
        Nc,
        Mc,
        Sc,
        data=metacont,
        studlab=paste(Author),
        comb.fixed = FALSE,
        comb.random = TRUE,
        method.tau = "SJ",
        hakn = TRUE,
        prediction=TRUE,
        sm="SMD")
m.hksj.raw

```

## Meta-Analysis with binary outcomes {#binary}

In some cases, you will work with **binary outcome data** (e.g., dead/alive, Depressive Disorder/no Depressive Disorder) instead of continuous data. In such a case, you will probably be more interested in outcomes like the pooled **Odd's Ratio** or the **Relative Risk Reduction**.

Here, have two options again:

* **The effect sizes are already calculated**. In this case, we can use the `metagen` function as we did before (see [Chapter 4.1](#fixed) and [Chapter 4.2](#random)). The calculated effect `TE` then describes the Odds Ratio, or whatever binary outcome we calculated previously for our data.
* **We only have the raw outcome data**. If this is the case, we will have to use the `meta::metabin` function instead. We'll show you how to do this now.

**For meta-analyses of binary outcomes, we need our data in the following format:**

```{r,echo=FALSE}
library(kableExtra)
Package<-c("Author","Ee","Ne","Ec","Nc","Subgroup")
Description<-c(
  
"This signifies the column for the study label (i.e., the first author)",

"Number of events in the experimental treatment arm",

"Number of participants in the experimental treatment arm",

"Number of events in the control arm",

"Number of participants in the control arm",


"This is the label for one of your Subgroup codes. It's not that important how you name it, so you can give it a more informative name (e.g. population). In this column, each study should then be given an subgroup code, which should be exactly the same for each subgroup, including upper/lowercase letters. Of course, you can also include more than one subgroup column with different subgroup codings, but the column name has to be unique")
m<-data.frame(Package,Description)
names<-c("Column", "Description")
colnames(m)<-names
kable(m)
```

I'll use my dataset `binarydata`, which also has this format
```{r}
load("binarydata.RData")
str(binarydata)
```

The other parameters are the as the ones we used in the meta-analyses with continuous outcome data, with two exceptions:

* **sm**: As we want to have a pooled effect for binary data, we have to choose another summary measure now. We can choose from **"OR"** (Odds Ratio), **"RR"** (Risk Ratio), or **RD** (Risk Difference), among other things
* **incr**. This lets us define if and how we want **conitinuity correction** to be performed. Such a correction if necessary cases where one of the cells in your data is zero (e.g., because no one in the intervention arm died). This can be a frequent phenomenon in some context, and **distorts our effect size estimates**. By default, the `metabin` function adds the value **0.5** in all cells were N is zero [@gart1967bias]. This value can be changed using the `incr`-function (e.g., `incr=0.1`). If your trial arms are very uneven in terms of their total $n$, we can also use the **treatment arm continuity correction** [@j2004add]. This can be done by using `incr="TACC"`.

**Here's the code for a meta-analysis with raw binary data**
I have decided to run a random-effect-model meta-analysis. I want the summary measure to be the Risk Ratio (RR).

```{r}
m.bin<-metabin(Ee,
        Ne,
        Ec,
        Nc,
        data=binarydata,
        studlab=paste(Author),
        comb.fixed = FALSE,
        comb.random = TRUE,
        method.tau = "SJ",
        hakn = TRUE,
        prediction=TRUE,
        incr=0.1,
        sm="RR")
m.bin
```




